{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90627ff8-db92-4b5d-a497-1bfc04ac39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df.iloc[:, :-1]), columns=data.feature_names)\n",
    "df_scaled['target'] = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f17874-b1ef-4574-9dc2-f0772f2c91fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9736842105263158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled.iloc[:, :-1], df_scaled['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d774c7e6-4175-4c3b-a95c-9828e692b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885a7fe6-20cb-4b39-9cb5-0d91085a9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db42e297-ef08-4813-a930-51216110fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d668820d-5023-4f8f-8a8f-4a6e490c8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # Using 5 neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7f482ce-a174-4d4a-926d-b3e2be9781a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to evaluate and store results\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test)  # Make predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"ðŸ”¹ {name} ðŸ”¹\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e581dc0b-5d41-48a1-bc19-34b0c75c377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Logistic Regression ðŸ”¹\n",
      "Accuracy: 0.9737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Decision Tree ðŸ”¹\n",
      "Accuracy: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Random Forest ðŸ”¹\n",
      "Accuracy: 0.9649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ SVM ðŸ”¹\n",
      "Accuracy: 0.9737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ k-NN ðŸ”¹\n",
      "Accuracy: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "evaluate_model(\"Logistic Regression\", LogisticRegression(), X_train, X_test, y_train, y_test)\n",
    "evaluate_model(\"Decision Tree\", DecisionTreeClassifier(), X_train, X_test, y_train, y_test)\n",
    "evaluate_model(\"Random Forest\", RandomForestClassifier(), X_train, X_test, y_train, y_test)\n",
    "evaluate_model(\"SVM\", SVC(), X_train, X_test, y_train, y_test)\n",
    "evaluate_model(\"k-NN\", KNeighborsClassifier(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28592980-18ef-484d-a2d9-ab0889efec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Model Performance Comparison ðŸ”¹\n",
      "                 Model  Accuracy\n",
      "0  Logistic Regression  0.973684\n",
      "3                  SVM  0.973684\n",
      "2        Random Forest  0.964912\n",
      "1        Decision Tree  0.947368\n",
      "4                 k-NN  0.947368\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(list(model_results.items()), columns=[\"Model\", \"Accuracy\"])\n",
    "df_results = df_results.sort_values(by=\"Accuracy\", ascending=False)\n",
    "\n",
    "print(\"ðŸ”¹ Model Performance Comparison ðŸ”¹\")\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
